{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandas imports / aliases / helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "from pathlib import Path\n",
    "\n",
    "from numpy import nan, array, ndarray\n",
    "from os import remove\n",
    "from os.path import exists, isdir, splitext\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import \\\n",
    "    concat, DataFrame as DF, Series, \\\n",
    "    isna, \\\n",
    "    read_csv, read_excel, read_json, read_parquet, read_sql, read_sql_query, read_sql_table, \\\n",
    "    date_range, to_datetime as to_dt, Timedelta as Î”, NaT\n",
    "\n",
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display(r=None, c=None):\n",
    "    '''Set the default number of rows and columns for Pandas to display'''\n",
    "    if r:\n",
    "        pd.options.display.max_rows = r\n",
    "    if c:\n",
    "        pd.options.display.max_columns = c\n",
    "\n",
    "display(100, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concat / Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sxs(*dfs, **kwargs):\n",
    "    '''Concat some DataFrames \"side by side\"'''\n",
    "    return concat(dfs, axis=1, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_counts(series, ascending = None):\n",
    "    df_new = DF(series.value_counts())\n",
    "    column_name = df_new.columns[0]\n",
    "    df_new['index'] = df_new.index\n",
    "    ascending = [False,True] if ascending is None else ascending\n",
    "    return df_new.sort_values(by = [column_name,'index'], ascending = ascending)[column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_by_series(df, series):\n",
    "    tmp_col_name = series.name or '_'\n",
    "    while tmp_col_name in df:\n",
    "        tmp_col_name=f'_{tmp_col_name}'\n",
    "\n",
    "    if isinstance(series, str):\n",
    "        series = df[series]\n",
    "\n",
    "    df2 = df.copy()\n",
    "    df2[tmp_col_name] = series\n",
    "    return \\\n",
    "        df2 \\\n",
    "        .sort_values(tmp_col_name) \\\n",
    "        .drop(columns=tmp_col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_dict(path):\n",
    "    name = path.name\n",
    "    pieces = name.rsplit('.', 1)\n",
    "    if len(pieces) == 2:\n",
    "        base, xtn = pieces\n",
    "    else:\n",
    "        [ base ] = pieces\n",
    "        xtn = nan\n",
    "    \n",
    "    return dict(parent=path.parent, name=name, base=base, xtn=xtn, exists=path.exists())\n",
    "\n",
    "\n",
    "def annotate_files(files):\n",
    "    if files.empty:\n",
    "        dtypes = {\n",
    "            'parent': object,\n",
    "            'name': str,\n",
    "            'base': str,\n",
    "            'xtn': str,\n",
    "            'exists': bool,\n",
    "        }\n",
    "        return DF([], index=files, columns=dtypes.keys()).astype(dtypes)\n",
    "    else:\n",
    "        extra = files.apply(file_dict).apply(Series)\n",
    "        files = sxs(files, extra).set_index(files.name)\n",
    "        return files\n",
    "\n",
    "\n",
    "def load_files(files, glob=None, name=None):\n",
    "    if isinstance(files, Path):\n",
    "        name = name or 'path'\n",
    "        files = Series(files.glob(glob), name=name, dtype=object)\n",
    "    else:\n",
    "        name = name or files.name\n",
    "    files = annotate_files(files)\n",
    "    return files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_date(s):\n",
    "    d = parse(s).date()\n",
    "    return dict(date=d, year=d.year, month=d.month, day=d.day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_parquet(df, out_path, verify_extension=True, *args, **kwargs):\n",
    "    if verify_extension:\n",
    "        _, extension = splitext(out_path)\n",
    "        if extension != '.parquet':\n",
    "            raise Exception(f\"Refusing to write parquet dataset to non-parquet path {out_path}\")\n",
    "\n",
    "    if exists(out_path):\n",
    "        if isdir(out_path):\n",
    "            rmtree(out_path)\n",
    "        else:\n",
    "            remove(out_path)\n",
    "\n",
    "    return df.to_parquet(out_path, *args, **kwargs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
